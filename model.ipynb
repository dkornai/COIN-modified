{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; np.set_printoptions(linewidth = 150, suppress=True)\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.distance import cdist\n",
    "from generate import generate_TPM, generate_contexts_cues\n",
    "from particle import Particle, resample_particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A modified COIN model\n",
    "## Introduction\n",
    "\n",
    "The COIN model was originally introduced by [Heald et. al 2023](https://doi.org/10.1038/s41586-021-04129-3) to study motor learning.\n",
    "\n",
    "This project utilises a version that is simplified by removing latent states, and modified by making the cue multinomial.\n",
    "\n",
    "### The modified COIN generative model\n",
    "Contexts occur with frequency $\\beta$.  \n",
    "At each time step, the latent context variable evolves as a Markov process according to the transition probability matrix (TPM) $\\Pi$.\n",
    "$$\n",
    "\\Pi=\n",
    "\\begin{bmatrix}\n",
    "p({c_{t}=1|c_{t-1}=1}) & p({c_{t}=2|c_{t-1}=1}) & ... \\\\\n",
    "p({c_{t}=1|c_{t-1}=2}) & p({c_{t}=2|c_{t-1}=2}) & ... \\\\\n",
    "... & ... & p({c_{t}=j|c_{t-1}=i})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Each context is associated with a given row of a cue emission matrix (CEM)$\\Phi$, \n",
    "$$\\Phi=\n",
    "\\begin{bmatrix}\n",
    "p(q_1=1|c=1) & p(q_2=1|c=1) & ... \\\\\n",
    "p(q_1=1|c=2) & p(q_2=1|c=2) & ... \\\\\n",
    "p(q_1=1|c=3) & p(q_2=1|c=3) & ... \\\\\n",
    "... & ... & p(q_i=1|c=j) \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "such that at each time step, a binary cue vector (e.g. $q = \\{ 1, 0, 0, 0, 1, 0\\}$) corresponding to that context is emitted.\n",
    "\n",
    "### Inference under the modified model\n",
    "\n",
    "The goal of the learner is to compute the joint posterior $p(\\Theta_t | q_{1:t})$ of quantities $\\Theta_t = \\{c_t, \\beta, \\Pi, \\Phi\\}$ that are not observed by the learner: the current context $c_t$, the global context frequencies $\\beta$, the TPM $\\Pi$, and the CEM $\\Phi$. \n",
    "\n",
    "This is accomplished by using particle learning, which is detailed for the original model in section 2.3 of the [supplementary materials](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-04129-3/MediaObjects/41586_2021_4129_MOESM1_ESM.pdf). Here is a short outline of the process:\n",
    "\n",
    "The essential state vector of each particle $z_t$ is composed of:\n",
    "- context $c_t$\n",
    "- the sufficient statistics $\\Theta^s$ \n",
    "    - $n_t$ the context transition count matrix\n",
    "    - $n_q$ the cue emission count matrix, with element $C_q[context = j,i]$ being the number of times the $i$-th element of the cue vector was 1 for context $j$\n",
    "- the parameters $\\Theta$ are composed of\n",
    "    - $\\hat{\\beta}$ the estimated global context frequencies\n",
    "    - $\\hat{\\Phi.}$ the estimated cue emission matrix\n",
    "\n",
    "The distribution of essential state vectors is evolved over time with each cue by repeating the following steps:\n",
    "\n",
    "1) #### Resampling\n",
    "    First, particles are sampled with replacement according to weights $w_t$ proportional to the predictive distribution $w_t \\propto \\hat{p}(q_t | z_{t-1})$\n",
    "    which can be decomposed into the expected local context transition probability and the local cue probability: $$\\sum_{j=1}^{C+1}p(c_t = j | z_{t-1})p(q_t|c_t = j,z_{t-1})$$\n",
    "    with $p(c_t|z_{t-1})$ being given by eq. S15 of the supplementary materials, and $p(q_t|c_t = j,z_{t-1})$ modified to be the probability of observing a binary cue vector, given each context specific row of the estimated CEM $\\hat{\\Phi}$\n",
    "    $$ p(q_t|c_t = j,z_{t-1}) = \\prod_{i=1}^{l} \\hat{\\Phi}[j,i]^{q_i} \\cdot (1 - \\hat{\\Phi}[j,i])^{1 - q_i} $$\n",
    "\n",
    "2) #### Propagation\n",
    "    Next, the latent context variable $c_t$ is propagated conditioned on the predictive distribution $\\hat{p}(q_t | z_{t-1})$, and the sufficient statistics $\\Theta^s$ (context transition counts, context counts, cue observation counts) are incremented.\n",
    "\n",
    "3) #### Parameter sampling\n",
    "    To maintain diversity in parameters over the particles, the parameters $\\beta$ and $\\hat{\\Phi}$ are resampled. $\\beta$ is resampled according to eqs. S25-28 of the supplementary materials, while each entry of $\\hat{\\Phi}$ is sampled from:\n",
    "    $$\n",
    "    \\hat{\\Phi}[context = j, i] \\sim Beta(a = n_q[j,i] + \\hat{\\Phi}[j, i], b = n_c[j] - n_q[j,i] + (1 - \\hat{\\Phi}[j, i]))\n",
    "    $$\n",
    "\n",
    "Finally, to estimate the CEM, the distribution of $\\hat{\\Phi}$ over particles is averaged, To estimate the full TPM $\\Pi$, eq. S29 is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "#### Generate data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global context probabilities:\n",
      "[0.27604136 0.27577765 0.25739515 0.19078585]\n",
      "True context transition probability matrix:\n",
      "[[0.04207087 0.95434098 0.0035598  0.00002836]\n",
      " [0.22130562 0.2036342  0.0384161  0.53664408]\n",
      " [0.16695486 0.00529811 0.66396899 0.16377804]\n",
      " [0.13138131 0.14672764 0.72187145 0.0000196 ]]\n",
      "Element 0: Frequency 281\n",
      "Element 1: Frequency 428\n",
      "Element 2: Frequency 903\n",
      "Element 3: Frequency 388\n"
     ]
    }
   ],
   "source": [
    "cue_dim = 8         # Dimensionality of binary cue vector\n",
    "n_contexts_true = 4 # Number of true contexts\n",
    "\n",
    "# Set hyperparameters of the generative model\n",
    "hyp_gamma = 5.0     # Controls the effective number of contexts, use larger values for more low-prob. contexts\n",
    "hyp_alpha = 3.0     # Controls the resemblence of local transition prob.s to global transition prob.s\n",
    "hyp_kappa = 1.5     # Controls the rate of self-transitions, use larger values for more self-transitions\n",
    "\n",
    "# Generate the true TPM\n",
    "true_TPM = generate_TPM(n_contexts_true, hyp_gamma, hyp_alpha, hyp_kappa)\n",
    "\n",
    "# Specify the true CEM\n",
    "true_CEM = [\n",
    "    [1, 1, 0, 0, 0, 0, 1, 1],\n",
    "    [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 1, 1, 1, 1]\n",
    "]\n",
    "true_CEM = np.array(true_CEM, dtype = np.int64)\n",
    "\n",
    "# Simulate data according to the generative model\n",
    "t_steps = 2000\n",
    "contexts, cues = generate_contexts_cues(true_TPM, true_CEM, t_steps)\n",
    "\n",
    "# Print the context frequencies in the generated data\n",
    "unique_elements, counts = np.unique(contexts, return_counts=True)\n",
    "element_frequency = dict(zip(unique_elements, counts))\n",
    "for element, frequency in element_frequency.items():\n",
    "    print(f\"Element {element}: Frequency {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use particle learning to infer the TPM and CEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> processing cue 2000 of 2000     \r"
     ]
    }
   ],
   "source": [
    "# Specify the hyperparameters of the learner\n",
    "hyp_lambda = 0.5    # Prior probability of a given cue vector element being 1\n",
    "model_hyperparam = {\n",
    "    'hyp_gamma' : hyp_gamma,\n",
    "    'hyp_alpha' : hyp_alpha,\n",
    "    'hyp_kappa' : hyp_kappa,\n",
    "    'hyp_lambda': hyp_lambda,\n",
    "    }\n",
    "\n",
    "# Set up the ensemble of particles\n",
    "particles = [Particle(cue_dim=cue_dim, n_contexts_init=0, hyperparam=model_hyperparam) for _ in range(100)]\n",
    "\n",
    "# Learn using data\n",
    "for i, cue in enumerate(cues):\n",
    "    print(f'>> processing cue {i+1} of {len(cues)}     ', end = '\\r')    \n",
    "    \n",
    "    # Calculate the posterior proportional, and sum over contexts to get the weights\n",
    "    weights = []\n",
    "    for particle in particles:\n",
    "        particle.calculate_posterior_prop(cue)\n",
    "        particle.calculate_weight()\n",
    "        weights.append(particle.weight)\n",
    "    \n",
    "    particles = resample_particles(particles, weights)\n",
    "    \n",
    "    # Propagate the context and sufficient statistics\n",
    "    for particle in particles:\n",
    "        particle.propagate_context()\n",
    "        particle.propagate_sufficient_statistics(cue)\n",
    "\n",
    "    # Sample the parameters to generate diversity\n",
    "    for particle in particles:\n",
    "        particle.sample_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the estimated and true TPM and CEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_CEM:\n",
      "[[1 1 0 0 0 0 1 1]\n",
      " [0 0 0 0 1 1 1 1]\n",
      " [0 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 1 1 1 1]]\n",
      "Estimated CEM:\n",
      "[[1. 1. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 1. 1. 1.]]\n",
      "True TPM:\n",
      "[[0.04 0.95 0.   0.  ]\n",
      " [0.22 0.2  0.04 0.54]\n",
      " [0.17 0.01 0.66 0.16]\n",
      " [0.13 0.15 0.72 0.  ]]\n",
      "Estimated TPM:\n",
      "[[0.02 0.96 0.   0.  ]\n",
      " [0.2  0.21 0.03 0.56]\n",
      " [0.16 0.   0.67 0.16]\n",
      " [0.11 0.16 0.72 0.01]]\n"
     ]
    }
   ],
   "source": [
    "# Estimate the CEM \n",
    "list_cue_emission_matrix = np.array([particle.get_theta_CEM() for particle in particles])\n",
    "estimated_CEM = np.round(np.average(list_cue_emission_matrix, axis = 0), 2)\n",
    "\n",
    "# Use the similarity between rows of the estimated and true CEM to reorder rows in the output data\n",
    "distance_matrix = cdist(true_CEM, estimated_CEM, 'euclidean')\n",
    "closest_indices = np.argmin(distance_matrix, axis=1)\n",
    "\n",
    "# Compare the estimate and the true CEM\n",
    "print(\"true_CEM:\")\n",
    "print(true_CEM)\n",
    "print(\"Estimated CEM:\")\n",
    "print(estimated_CEM[closest_indices])\n",
    "\n",
    "# Estimate the TPM\n",
    "n_contexts_inferred = len(particles[0].get_ss_n_c())\n",
    "exp_TPM = np.zeros((n_contexts_inferred, n_contexts_inferred))\n",
    "for j in range(n_contexts_inferred):\n",
    "    for k in range(n_contexts_inferred):\n",
    "        for particle in particles:\n",
    "            exp_TPM[j,k] += (hyp_alpha*particle.get_theta_beta_c()[k] + hyp_kappa*(j == k) + particle.get_ss_n_t()[j,k])/(hyp_alpha+hyp_kappa+particle.get_ss_n_c()[j])\n",
    "exp_TPM /= len(particles)\n",
    "\n",
    "# Compare the estimated and the true TPM\n",
    "print(\"True TPM:\")\n",
    "print(np.round(true_TPM, 2))\n",
    "print(\"Estimated TPM:\")\n",
    "print(np.round(exp_TPM[closest_indices][:,closest_indices], 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
