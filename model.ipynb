{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; np.set_printoptions(linewidth = 150, suppress=True)\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.distance import cdist\n",
    "from generate import generate_TPM, generate_contexts_cues\n",
    "from particle import Particle, resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A modified COIN model\n",
    "## Introduction\n",
    "\n",
    "The COIN model was originally introduced by [Heald et. al 2023](https://doi.org/10.1038/s41586-021-04129-3) to study motor learning.\n",
    "\n",
    "This project utilises a version that is simplified by removing latent states, and modified by making the cue multinomial.\n",
    "\n",
    "### The modified COIN generative model\n",
    "Contexts occur with frequency $\\beta$.  \n",
    "At each time step, the latent context variable evolves as a Markov process according to the transition probability matrix (TPM) $\\Pi$.\n",
    "$$\n",
    "\\Pi=\n",
    "\\begin{bmatrix}\n",
    "p({c_{t}=1|c_{t-1}=1}) & p({c_{t}=2|c_{t-1}=1}) & ... \\\\\n",
    "p({c_{t}=1|c_{t-1}=2}) & p({c_{t}=2|c_{t-1}=2}) & ... \\\\\n",
    "... & ... & p({c_{t}=j|c_{t-1}=i})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Each context is associated with a given row of a cue emission matrix (CEM)$\\Phi$, \n",
    "$$\\Phi=\n",
    "\\begin{bmatrix}\n",
    "p(q_1=1|c=1) & p(q_2=1|c=1) & ... \\\\\n",
    "p(q_1=1|c=2) & p(q_2=1|c=2) & ... \\\\\n",
    "p(q_1=1|c=3) & p(q_2=1|c=3) & ... \\\\\n",
    "... & ... & p(q_i=1|c=j) \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "such that at each time step, a binary cue vector (e.g. $q = \\{ 1, 0, 0, 0, 1, 0\\}$) corresponding to that context is emitted.\n",
    "\n",
    "### Inference under the modified model\n",
    "\n",
    "The goal of the learner is to compute the joint posterior $p(\\Theta_t | q_{1:t})$ of quantities $\\Theta_t = \\{c_t, \\beta, \\Pi, \\Phi\\}$ that are not observed by the learner: the current context $c_t$, the global context frequencies $\\beta$, the TPM $\\Pi$, and the CEM $\\Phi$. \n",
    "\n",
    "This is accomplished by using particle learning, which is detailed for the original model in section 2.3 of the [supplementary materials](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-04129-3/MediaObjects/41586_2021_4129_MOESM1_ESM.pdf). Here is a short outline of the process:\n",
    "\n",
    "The essential state vector of each particle $z_t$ is composed of:\n",
    "- context $c_t$\n",
    "- the sufficient statistics $\\Theta^s$ \n",
    "    - $C_c$ the count of how many times each context was observed\n",
    "    - $C_t$ the context transition count matrix\n",
    "    - $C_q$ the cue emission count matrix, with element $C_q[context = j,i]$ being the number of times the $i$-th element of the cue vector was 1 for context $j$\n",
    "- the parameters $\\Theta$ are composed of\n",
    "    - $\\hat{\\beta}$ the estimated global context frequencies\n",
    "    - $\\hat{\\Phi.}$ the estimated cue emission matrix\n",
    "\n",
    "The distribution of essential state vectors is evolved over time with each cue by repeating the following steps:\n",
    "\n",
    "1) #### Resampling\n",
    "    First, particles are sampled with replacement according to weights $w_t$ proportional to the predictive distribution $w_t \\propto \\hat{p}(q_t | z_{t-1})$\n",
    "    which can be decomposed into the expected local context transition probability and the local cue probability: $$\\sum_{j=1}^{C+1}p(c_t = j | z_{t-1})p(q_t|c_t = j,z_{t-1})$$\n",
    "    with $p(c_t|z_{t-1})$ being given by eq. S15 of the supplementary materials, and $p(q_t|c_t = j,z_{t-1})$ modified to be the probability of observing a binary cue vector, given each context specific row of the estimated CEM $\\hat{\\Phi}$\n",
    "    $$ p(q_t|c_t = j,z_{t-1}) = \\prod_{i=1}^{l} \\hat{\\Phi}[j,i]^{q_i} \\cdot (1 - \\hat{\\Phi}[j,i])^{1 - q_i} $$\n",
    "\n",
    "2) #### Propagation\n",
    "    Next, the latent context variable $c_t$ is propagated conditioned on the predictive distribution $\\hat{p}(q_t | z_{t-1})$, and the sufficient statistics $\\Theta^s$ (context transition counts, context counts, cue observation counts) are incremented.\n",
    "\n",
    "3) #### Parameter sampling\n",
    "    To maintain diversity in parameters over the particles, the parameters $\\beta$ and $\\hat{\\Phi}$ are resampled. $\\beta$ is resampled according to eqs. S25-28 of the supplementary materials, while each entry of $\\hat{\\Phi}$ is sampled from:\n",
    "    $$\n",
    "    \\hat{\\Phi}[context = j, i] \\sim Beta(a = C_q[j,i] + \\hat{\\Phi}[j, i], b = C_c[j] - C_q[j,i] + (1 - \\hat{\\Phi}[j, i]))\n",
    "    $$\n",
    "\n",
    "Finally, to estimate the CEM, the distribution of \\hat{\\Phi} over particles is averaged, To estimate the full TPM $\\Pi$, eq. S29 is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "#### Generate data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global context probabilities:\n",
      "[0.53071743 0.04051346 0.21692036 0.21184875]\n",
      "True context transition probability matrix:\n",
      "[[0.86239456 0.10909311 0.02850528 0.00000705]\n",
      " [0.11357978 0.33306852 0.33658356 0.21676815]\n",
      " [0.08137829 0.         0.75657932 0.1620424 ]\n",
      " [0.23946346 0.         0.00004106 0.76049548]]\n",
      "Element 0: Frequency 466\n",
      "Element 1: Frequency 105\n",
      "Element 2: Frequency 187\n",
      "Element 3: Frequency 242\n"
     ]
    }
   ],
   "source": [
    "input_dim = 6 # Dimensionality of binary cue vector\n",
    "n_contexts_true = 4 # Number of true contexts\n",
    "\n",
    "# Set hyperparameters of the generative model\n",
    "h_gamma_c = 1.0\n",
    "h_alpha_c = 5.0\n",
    "h_kappa_c = 2.0\n",
    "\n",
    "# Generate the true TPM\n",
    "true_TPM = generate_TPM(input_dim, n_contexts_true, h_gamma_c, h_alpha_c, h_kappa_c)\n",
    "\n",
    "# Specify the true CEM\n",
    "true_CEM = [\n",
    "    [1, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 1],\n",
    "    [0, 0, 1, 1, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 1]\n",
    "]\n",
    "true_CEM = np.array(true_CEM, dtype = np.int64)\n",
    "\n",
    "# Simulate data according to the generative model\n",
    "t_steps = 1000\n",
    "contexts, cues = generate_contexts_cues(true_TPM, true_CEM, t_steps)\n",
    "\n",
    "# Print the context frequencies in the generated data\n",
    "unique_elements, counts = np.unique(contexts, return_counts=True)\n",
    "element_frequency = dict(zip(unique_elements, counts))\n",
    "for element, frequency in element_frequency.items():\n",
    "    print(f\"Element {element}: Frequency {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use particle learning to infer the TPM and CEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> processing cue 999 of 1000     \r"
     ]
    }
   ],
   "source": [
    "# Specify the hyperparameters of the learner\n",
    "model_hyperparam = {\n",
    "    'h_gamma_c' : h_gamma_c,\n",
    "    'h_alpha_c' : h_alpha_c,\n",
    "    'h_kappa_c' : h_kappa_c\n",
    "    }\n",
    "\n",
    "# Set up the ensemble of particles\n",
    "particles = [Particle(input_dim=input_dim, n_contexts_init=0, hyperparam=model_hyperparam) for _ in range(20)]\n",
    "\n",
    "# Learn using data\n",
    "for i, cue in enumerate(cues):\n",
    "    print(f'>> processing cue {i} of {len(cues)}     ', end = '\\r')    \n",
    "    \n",
    "    # Calculate the responsibility, and use this to resample the particles\n",
    "    weights = []\n",
    "    for particle in particles:\n",
    "        particle.calculate_joint(cue)\n",
    "        particle.calculate_responsibility()\n",
    "        weights.append(particle.get_responsibility())\n",
    "    \n",
    "    particles = resample(particles, weights)\n",
    "    \n",
    "    # Propagate the context and sufficient statistics\n",
    "    for particle in particles:\n",
    "        particle.propagate_context()\n",
    "        particle.propagate_sufficient_statistics(cue)\n",
    "\n",
    "    # Sample the parameters to generate diversity\n",
    "    for particle in particles:\n",
    "        particle.sample_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the estimated and true TPM and CEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_CEM:\n",
      "[[1 1 0 0 0 0]\n",
      " [0 0 0 0 1 1]\n",
      " [0 0 1 1 0 0]\n",
      " [1 0 0 0 0 1]]\n",
      "Estimated CEM:\n",
      "[[1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]]\n",
      "True TPM:\n",
      "[[0.86 0.11 0.03 0.  ]\n",
      " [0.11 0.33 0.34 0.22]\n",
      " [0.08 0.   0.76 0.16]\n",
      " [0.24 0.   0.   0.76]]\n",
      "Estimated TPM:\n",
      "[[0.81 0.15 0.04 0.  ]\n",
      " [0.12 0.34 0.36 0.15]\n",
      " [0.08 0.01 0.7  0.2 ]\n",
      " [0.23 0.   0.   0.75]]\n"
     ]
    }
   ],
   "source": [
    "# Estimate the CEM \n",
    "list_cue_emission_matrix = np.array([particle.get_cue_emission_matrix() for particle in particles])\n",
    "estimated_CEM = np.round(np.average(list_cue_emission_matrix, axis = 0), 1)\n",
    "\n",
    "# Use the similarity between rows of the estimated and true CEM to reorder rows in the output data\n",
    "distance_matrix = cdist(true_CEM, estimated_CEM, 'euclidean')\n",
    "closest_indices = np.argmin(distance_matrix, axis=1)\n",
    "\n",
    "# Compare the estimate and the true CEM\n",
    "print(\"true_CEM:\")\n",
    "print(true_CEM)\n",
    "print(\"Estimated CEM:\")\n",
    "print(estimated_CEM[closest_indices])\n",
    "\n",
    "# Estimate the TPM\n",
    "n_contexts_inferred = len(particles[0].get_context_counts())\n",
    "exp_TPM = np.zeros((n_contexts_inferred, n_contexts_inferred))\n",
    "for j in range(n_contexts_inferred):\n",
    "    for k in range(n_contexts_inferred):\n",
    "        for particle in particles:\n",
    "            exp_TPM[j,k] += (h_alpha_c*particle.get_global_context_prob()[k] + h_kappa_c*(j == k) + particle.get_transition_counts()[j,k])/(h_alpha_c+h_kappa_c+particle.get_context_counts()[j])\n",
    "exp_TPM /= len(particles)\n",
    "\n",
    "# Compare the estimated and the true TPM\n",
    "print(\"True TPM:\")\n",
    "print(np.round(true_TPM, 2))\n",
    "print(\"Estimated TPM:\")\n",
    "print(np.round(exp_TPM[closest_indices][:,closest_indices], 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
